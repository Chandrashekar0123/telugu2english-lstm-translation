# üåê Telugu to English Neural Machine Translator üáÆüá≥‚û°Ô∏èüá¨üáß
> A Sequence-to-Sequence (Seq2Seq) LSTM-based Neural Machine Translation (NMT) model for translating Telugu sentences into English using TensorFlow and Keras.

![Python](https://img.shields.io/badge/Python-3.9-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0%2B-orange)
![Status](https://img.shields.io/badge/Status-Prototype-yellow)
![License](https://img.shields.io/badge/License-MIT-green)

---

## üß† Problem Statement

Telugu is one of the major languages spoken in India but lacks robust translation tools when compared to high-resource languages like English. The goal of this project is to create a basic Neural Machine Translation (NMT) system that can convert Telugu sentences into English using a deep learning model. This serves as a foundational prototype for building a larger, production-grade translation tool.

---

## ‚ú® Features

- ‚úÖ End-to-End Telugu ‚û°Ô∏è English translation system
- üìö Custom tokenizer with <start> and <end> token handling
- üîÅ LSTM-based Seq2Seq architecture
- üß™ Inference with greedy decoding
- üõ†Ô∏è Easily customizable and extensible
- ‚öôÔ∏è Trained on small manually-curated dataset (demo-friendly)

---

## üõ†Ô∏è Technologies Used

| Technology       | Description                          |
|------------------|--------------------------------------|
| Python           | Programming language                 |
| TensorFlow/Keras | Deep learning framework              |
| NumPy            | Numerical operations                 |
| Jupyter Notebook | Development interface (optional)     |
| Matplotlib       | (Optional) For plotting loss curves  |

---

## üìö Dataset

This project uses a **manually created small dataset** of parallel Telugu-English sentences for demonstration purposes. You can easily scale this up using:

- [AI4Bharat](https://ai4bharat.iitm.ac.in/indictrans/)
- [Tatoeba Project](https://tatoeba.org/)
- [OpenSubtitles via OPUS](https://opus.nlpl.eu/OpenSubtitles-v2018.php)

Sample pairs:

| Telugu                             | English                      |
|------------------------------------|-------------------------------|
| ‡∞®‡±á‡∞®‡±Å ‡∞á‡∞Ç‡∞ó‡±ç‡∞≤‡±Ä‡∞∑‡±ç ‡∞Æ‡∞æ‡∞ü‡±ç‡∞≤‡∞æ‡∞°‡∞ó‡∞≤‡±Å‡∞ó‡±Å‡∞§‡∞æ‡∞®‡±Å     | I can speak English           |
| ‡∞à ‡∞∞‡±ã‡∞ú‡±Å ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞Æ‡∞Ç‡∞ö‡∞ø‡∞¶‡∞ø                 | Today is very good            |
| ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?                 | How are you?                  |

---

## üîç Procedure

1. Add `<start>` and `<end>` tokens to target (English) sentences
2. Tokenize both Telugu (source) and English (target) sentences
3. Pad sequences to a uniform length
4. Build a Seq2Seq model with LSTM encoder and decoder
5. Train the model using categorical crossentropy
6. Use encoder-decoder inference to translate new Telugu inputs

---


---

## üìà Model Architecture

- Encoder: Embedding ‚Üí LSTM ‚Üí Hidden States (h, c)
- Decoder: Embedding ‚Üí LSTM (initial state = encoder states) ‚Üí Dense (Softmax)
- Loss: Categorical Crossentropy
- Optimizer: Adam

---

