# ğŸŒ Telugu to English Neural Machine Translator ğŸ‡®ğŸ‡³â¡ï¸ğŸ‡¬ğŸ‡§
> A Sequence-to-Sequence (Seq2Seq) LSTM-based Neural Machine Translation (NMT) model for translating Telugu sentences into English using TensorFlow and Keras.

![Python](https://img.shields.io/badge/Python-3.9-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0%2B-orange)


---

## ğŸ§  Problem Statement

Telugu is one of the major languages spoken in India but lacks robust translation tools when compared to high-resource languages like English. The goal of this project is to create a basic Neural Machine Translation (NMT) system that can convert Telugu sentences into English using a deep learning model. This serves as a foundational prototype for building a larger, production-grade translation tool.

---

## âœ¨ Features

- âœ… End-to-End Telugu â¡ï¸ English translation system
- ğŸ“š Custom tokenizer with <start> and <end> token handling
- ğŸ” LSTM-based Seq2Seq architecture
- ğŸ§ª Inference with greedy decoding
- ğŸ› ï¸ Easily customizable and extensible
- âš™ï¸ Trained on small manually-curated dataset (demo-friendly)

---

## ğŸ› ï¸ Technologies Used

| Technology       | Description                          |
|------------------|--------------------------------------|
| Python           | Programming language                 |
| TensorFlow/Keras | Deep learning framework              |
| NumPy            | Numerical operations                 |
| Jupyter Notebook | Development interface (optional)     |
| Matplotlib       | (Optional) For plotting loss curves  |

---

## ğŸ“š Dataset

Sample pairs:

| Telugu                             | English                      |
|------------------------------------|-------------------------------|
| à°¨à±‡à°¨à± à°‡à°‚à°—à±à°²à±€à°·à± à°®à°¾à°Ÿà±à°²à°¾à°¡à°—à°²à±à°—à±à°¤à°¾à°¨à±     | I can speak English           |
| à°ˆ à°°à±‹à°œà± à°šà°¾à°²à°¾ à°®à°‚à°šà°¿à°¦à°¿                 | Today is very good            |
| à°®à±€à°°à± à°à°²à°¾ à°‰à°¨à±à°¨à°¾à°°à±?                 | How are you?                  |

---

## ğŸ” Procedure

1. Add `<start>` and `<end>` tokens to target (English) sentences
2. Tokenize both Telugu (source) and English (target) sentences
3. Pad sequences to a uniform length
4. Build a Seq2Seq model with LSTM encoder and decoder
5. Train the model using categorical crossentropy
6. Use encoder-decoder inference to translate new Telugu inputs

---


---

## ğŸ“ˆ Model Architecture

- Encoder: Embedding â†’ LSTM â†’ Hidden States (h, c)
- Decoder: Embedding â†’ LSTM (initial state = encoder states) â†’ Dense (Softmax)
- Loss: Categorical Crossentropy
- Optimizer: Adam

---

